#!/usr/bin/env python3
"""
ã€guohxã€‘æ”¹è¿›çš„åˆ†æè„šæœ¬ï¼Œèƒ½å¤Ÿå¤„ç†ä¾èµ–coeditoræ¨¡å—çš„pickleæ–‡ä»¶
"""

import pickle
import sys
import os
from pathlib import Path

# ã€guohxã€‘æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°coeditoræ¨¡å—
sys.path.insert(0, 'src')

def improved_analyze():
    """ã€guohxã€‘æ”¹è¿›çš„æ•°æ®åˆ†æ"""
    
    # ã€guohxã€‘è®¾ç½®è¦åˆ†æçš„ç›®å½•è·¯å¾„
    processed_dir = Path("datasets_root/perm2k/processed/C3ProblemGenerator(VERSION=3.1, analyzer=())/")
    
    print("=" * 80)
    print("ã€guohxã€‘æ”¹è¿›çš„ PROCESSED æ•°æ®åˆ†æ")
    print("=" * 80)
    
    if not processed_dir.exists():
        print("âŒ ç›®å½•ä¸å­˜åœ¨")
        return
    
    # ã€guohxã€‘éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in processed_dir.iterdir():
        if file_path.is_file():
            print(f"\nğŸ“ æ–‡ä»¶: {file_path.name}")
            print(f"ğŸ“Š å¤§å°: {file_path.stat().st_size:,} bytes")
            print("-" * 60)
            
            try:
                # ã€guohxã€‘å°è¯•å¯¼å…¥coeditoræ¨¡å—
                try:
                    import coeditor
                    print("âœ… coeditoræ¨¡å—å¯¼å…¥æˆåŠŸ")
                except ImportError as e:
                    print(f"âš ï¸ coeditoræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                    print("ğŸ”„ å°è¯•ç›´æ¥è¯»å–pickle...")
                
                # ã€guohxã€‘è¯»å–pickleæ–‡ä»¶
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                
                print(f"ğŸ“‹ æ•°æ®ç±»å‹: {type(data)}")
                
                if isinstance(data, dict):
                    print(f"ğŸ”‘ å­—å…¸é”®: {list(data.keys())}")
                    total_problems = 0
                    
                    for key, value in data.items():
                        print(f"\n  ğŸ“‚ åˆ†é›†: {key}")
                        print(f"    ğŸ“Š ç±»å‹: {type(value)}")
                        print(f"    ğŸ“ˆ é•¿åº¦: {len(value) if hasattr(value, '__len__') else 'N/A'}")
                        
                        if hasattr(value, '__len__') and len(value) > 0:
                            total_problems += len(value)
                            
                            # ã€guohxã€‘åˆ†æç¬¬ä¸€ä¸ªé—®é¢˜
                            first_problem = value[0]
                            print(f"    ğŸ” ç¬¬ä¸€ä¸ªé—®é¢˜ç±»å‹: {type(first_problem)}")
                            print(f"    ğŸ“ ç±»å: {first_problem.__class__.__name__}")
                            
                            # ã€guohxã€‘æ˜¾ç¤ºå±æ€§
                            if hasattr(first_problem, '__dict__'):
                                attrs = list(first_problem.__dict__.keys())
                                print(f"    ğŸ“ å±æ€§æ•°é‡: {len(attrs)}")
                                print(f"    ğŸ“ å±æ€§åˆ—è¡¨: {attrs}")
                                
                                # ã€guohxã€‘æ˜¾ç¤ºä¸€äº›å…³é”®å±æ€§
                                key_attrs = ['repo_name', 'commit_hash', 'file_path', 'edit_type', 'pre_edit', 'post_edit', 'span', 'context']
                                for attr in key_attrs:
                                    if hasattr(first_problem, attr):
                                        attr_value = getattr(first_problem, attr)
                                        if isinstance(attr_value, str) and len(attr_value) > 100:
                                            attr_value = attr_value[:100] + "..."
                                        print(f"      {attr}: {attr_value}")
                                
                                # ã€guohxã€‘æ˜¾ç¤ºæ‰€æœ‰å±æ€§çš„è¯¦ç»†ä¿¡æ¯
                                print(f"\n    ğŸ“ æ‰€æœ‰å±æ€§è¯¦ç»†ä¿¡æ¯:")
                                for attr_name, attr_value in first_problem.__dict__.items():
                                    attr_type = type(attr_value).__name__
                                    if isinstance(attr_value, str):
                                        if len(attr_value) > 50:
                                            display_value = attr_value[:50] + "..."
                                        else:
                                            display_value = attr_value
                                    elif hasattr(attr_value, '__len__'):
                                        display_value = f"[é•¿åº¦: {len(attr_value)}]"
                                    else:
                                        display_value = str(attr_value)
                                    print(f"      {attr_name} ({attr_type}): {display_value}")
                
                elif isinstance(data, list):
                    print(f"ğŸ“ˆ åˆ—è¡¨é•¿åº¦: {len(data)}")
                    if len(data) > 0:
                        first_item = data[0]
                        print(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(first_item)}")
                        print(f"ğŸ“ ç±»å: {first_item.__class__.__name__}")
                        if hasattr(first_item, '__dict__'):
                            attrs = list(first_item.__dict__.keys())
                            print(f"ğŸ“ å±æ€§æ•°é‡: {len(attrs)}")
                            print(f"ğŸ“ å±æ€§åˆ—è¡¨: {attrs}")
                
                print(f"\nâœ… æ€»é—®é¢˜æ•°é‡: {total_problems if 'total_problems' in locals() else 'N/A'}")
                
            except Exception as e:
                print(f"âŒ è¯»å–é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                
                # ã€guohxã€‘å°è¯•æ›´ç®€å•çš„è¯»å–æ–¹å¼
                try:
                    print("ğŸ”„ å°è¯•ç®€å•è¯»å–...")
                    with open(file_path, 'rb') as f:
                        raw_data = f.read()
                    print(f"ğŸ“Š åŸå§‹æ•°æ®å¤§å°: {len(raw_data)} bytes")
                    print(f"ğŸ“Š å‰200å­—èŠ‚: {raw_data[:200]}")
                except Exception as e2:
                    print(f"âŒ ç®€å•è¯»å–ä¹Ÿå¤±è´¥: {e2}")
            
            print("=" * 80)

def analyze_specific_file(file_path_str):
    """ã€guohxã€‘åˆ†ææŒ‡å®šçš„å•ä¸ªæ–‡ä»¶"""
    
    file_path = Path(file_path_str)
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"\nğŸ” åˆ†ææŒ‡å®šæ–‡ä»¶: {file_path}")
    print("=" * 80)
    
    try:
        # ã€guohxã€‘å°è¯•å¯¼å…¥coeditoræ¨¡å—
        try:
            import coeditor
            print("âœ… coeditoræ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸ coeditoræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        
        # ã€guohxã€‘è¯»å–pickleæ–‡ä»¶
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"ğŸ“‹ æ•°æ®ç±»å‹: {type(data)}")
        
        if isinstance(data, dict):
            print(f"ğŸ”‘ å­—å…¸é”®: {list(data.keys())}")
            
            for key, value in data.items():
                print(f"\nğŸ“‚ åˆ†é›†: {key}")
                print(f"ğŸ“Š ç±»å‹: {type(value)}")
                print(f"ğŸ“ˆ é•¿åº¦: {len(value) if hasattr(value, '__len__') else 'N/A'}")
                
                if hasattr(value, '__len__') and len(value) > 0:
                    print(f"ğŸ” ç¬¬ä¸€ä¸ªé—®é¢˜ç±»å‹: {type(value[0])}")
                    print(f"ğŸ“ ç±»å: {value[0].__class__.__name__}")
                    
                    if hasattr(value[0], '__dict__'):
                        attrs = list(value[0].__dict__.keys())
                        print(f"ğŸ“ å±æ€§æ•°é‡: {len(attrs)}")
                        print(f"ğŸ“ å±æ€§åˆ—è¡¨: {attrs}")
        
        elif isinstance(data, list):
            print(f"ğŸ“ˆ åˆ—è¡¨é•¿åº¦: {len(data)}")
            if len(data) > 0:
                print(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(data[0])}")
                print(f"ğŸ“ ç±»å: {data[0].__class__.__name__}")
                
    except Exception as e:
        print(f"âŒ è¯»å–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ã€guohxã€‘åˆ†ææ‰€æœ‰æ–‡ä»¶
    improved_analyze()
    
    # ã€guohxã€‘åˆ†æç‰¹å®šæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    # analyze_specific_file("datasets_root/perm2k/processed/C3ProblemGenerator(VERSION=3.1, analyzer=())/deepseek-ai~DeepSeek-V3(1000, is_training=False)") 